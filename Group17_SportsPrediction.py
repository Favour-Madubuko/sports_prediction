# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jNlniEiG-tMd-FX7JfbN574NkPDIo__9
"""

# Importing libraries for use
import pandas as pd,numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Mounting the drive to access FIFA 21 Player Dataset
drive.mount('/content/drive')

# Reading data from the FIFA 21 Player Dataset
df=pd.read_csv("/content/drive/My Drive/AI/players_21.csv")

"""## **Data Cleaning**"""

# Dropping columns with 30% or more null values
df.dropna(thresh= 0.3 * len(df), axis=1, inplace=True)

df.head()

# Remove unwanted and irrelevant data
# Dropping Columns
for column in df:
  if "_id"  in column:
    df.drop(column,axis=1, inplace=True)
  if "url"  in column:
      df.drop(column,axis=1, inplace=True)


unnecessary=["real_face","ls","st","rs","short_name","long_name","lb","lcb","club_jersey_number","release_clause_eur","cb","rcb","rb","gk","dob","league_name","lcm","cm","rcm","rm","lwb","ldm","cdm","rdm","rwb","lb","club_contract_valid_until","lw","lf","cf","rf","rw","lam","cam","ram","lm","lcm"]

df=df.drop(columns=unnecessary)

df.head()
# columns_to_drop

# Getting numeric data to work on
numeric_df = df.select_dtypes(include='number')
numeric_df.isnull().sum()

# Cleaning the dataset of all null values
columns_with_nans = numeric_df.columns[numeric_df.isna().any()].tolist()
numeric_df=numeric_df.drop(columns=columns_with_nans)

# Filling all missing values using the median
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")
df_imputed = imputer.fit_transform(df[columns_with_nans])
df_imputed = pd.DataFrame(df_imputed, columns=columns_with_nans)

# Drop it from numeric dataset
numeric_df.loc[:,"power_long_shots":"mentality_penalties"]

# Adding imputed dataframe to numeric dataframe
numeric_df = pd.concat([numeric_df,df_imputed],axis=1)

#Confirming for the null values
numeric_df.isnull().sum()

"""**Working with non-numeric data**"""

# Working with only columns of the dataset that contain alphabets
alpha_df = df.select_dtypes(exclude='number')

# Checking for missing values
alpha_df.isnull().sum()

# Get all columns with Nans
columns_with_nans = alpha_df.columns[alpha_df.isna().any()].tolist()
df_imputed = alpha_df.copy()

# Dropping columns with null values(Nans) from the dataset
alpha_df = alpha_df.drop(columns_with_nans, axis=1)

# Create a SimpleImputer with the 'most_frequent' strategy
imputer = SimpleImputer(strategy='most_frequent')

# Impute missing values in the specified columns
df_imputed[columns_with_nans] = imputer.fit_transform(df[columns_with_nans])
df_imputed = pd.DataFrame(df_imputed, columns=columns_with_nans)

# Adding the imputed dataset back to the dataframe by concating
alpha_df=pd.concat([alpha_df,df_imputed],axis=1)
columns_with_nans

# Checking to confirm that there are no null values
alpha_df.isnull().sum()

"""**Label Encoding the 'alphabet' dataframe**"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Create a copy of the DataFrame to avoid modifying the original data
encoded_df = alpha_df.copy()

for col in alpha_df.columns:
    encoded_df[col] = label_encoder.fit_transform(alpha_df[col])

alpha_df=encoded_df

"""# **Working with the Cleaned Dataset**"""

# The cleaned data set is the relevantDataSet
relevantDataSet=pd.concat([alpha_df,numeric_df],axis=1)

"""*Find all the relevant data to the overall*

"""

# Calculate the correlation matrix for overall
correlation_matrix = relevantDataSet.corr()['overall']

# Filter the correlations that meet your criteria (greater than 0.48 or less than -0.48) when compared to the overall column
# Reason for this decision
# # We decided to use a correlation range between -0.48 and 0.48 because it indicates a moderate linear relationship.
# # This means that as one variable increases or decreases, the other tends to move in the same general direction, but not necessarily in a perfectly linear way.
# # We believe that a correlation of -0.48 to 0.48 may not be extremely strong. It still provides valuable information for predictive modeling.
# # It suggests that there is some degree of association between the variables, and this association can be useful for making predictions.
# # Also to prevent multicollinearity. Moderate correlations are less likely to cause multicollinearity issues while still offering predictive power.
high_correlations = correlation_matrix[(correlation_matrix > 0.48) | (correlation_matrix < -0.48)]
high_correlations

"""*Our Graphical representations*"""

#LINEPLOT TO REPRESENT CORRELATION

# Selecting the desired columns that tel
x = relevantDataSet['potential']
y = relevantDataSet['overall']

# Plotting
plt.plot(x, y, marker='o', linestyle='-', color='g', label='Data')

plt.xlabel('Player Potential')
plt.ylabel('Overall Performance')
plt.title('Line Plot showing Player Potential against Overall Performance')
plt.legend()
plt.grid(True)

plt.show()

# Interpretation: It is clearly seen that there is a strong positive linear relationship between the player potential and their potentials
# This means that a player with a low potential will directly have an overall low performance while a player with high potential will directly have an overall high performance

# HEATMAP TO REPRESENT CORRELATION
heatmap_correlation = relevantDataSet[high_correlations.index].corr()
round(heatmap_correlation,2)
plt.figure(figsize=(5,5))
sns.heatmap(heatmap_correlation);

# Interpretation based on real-life scenario:
# Forwards: Forwards are most likely to have the skill moves "dribbling" and "attacking finishing". This makes sense, as forwards are responsible for scoring goals, and these skill moves help them to get past defenders and create scoring opportunities.
# Midfielders: Midfielders are most likely to have the skill moves "dribbling" and "passing". This makes sense, as midfielders are responsible for connecting the defense and attack, and these skill moves help them to move the ball past defenders and create scoring opportunities for their teammates.
# Defenders: Defenders are most likely to have the skill moves "defending standing tackle" and "defending sliding tackle". This makes sense, as defenders are responsible for preventing the opposition from scoring goals, and these skill moves help them to win the ball back from attackers.
# It is also worth noting that the skill move "dribbling" is the most common skill move among players in all positions. This suggests that dribbling is a fundamental skill for all soccer players to have, regardless of their position.

# Overall, the heatmap provides a useful overview of the distribution of skill moves among soccer players in different positions.

"""# **Scaling the cleaned data set**"""

# Scaling the columns to make better to prediciton
sc = StandardScaler()

# Finding scaler of relevantDataSet values without 'overall' which is the dependent variable
scaled = sc.fit_transform(relevantDataSet[high_correlations.index].loc[:,"potential":])
xtrain21scaled=sc

# Create a new dataFrame of the scaled dataset with the high correlations
sub_set_data = pd.DataFrame(scaled,columns=relevantDataSet[high_correlations.index].loc[:,"potential":].columns)
sub_set_data

"""*Defining xtrain as x and ytrain as y for further training*"""

x = sub_set_data
y = relevantDataSet['overall']

"""# **Cleaning the test dataset for prediction from the FIFA 22  Player dataset**"""

# Generating the xtest and ytest

# Using the fifa players data for 2022
df2 = pd.read_csv("/content/drive/My Drive/AI/players_22.csv")
relevantDataSet2 = df2[high_correlations.index]

# Dropping columns with 30% or more null values
relevantDataSet2.dropna(thresh= 0.3 * len(df2), axis=1, inplace=True)

# WORKING WITH THE NUMBERS JUST AS THE FIFA 21 PLAYER DATASET
numeric_df = relevantDataSet2.select_dtypes(include='number')
columns_with_nans = numeric_df.columns[numeric_df.isna().any()].tolist()

# Dropping columns with null values (Nans)
numeric_df = numeric_df.drop(columns=columns_with_nans)
imputer = SimpleImputer(strategy="median")

# Imputing the data
df_imputed = imputer.fit_transform(relevantDataSet2[columns_with_nans])
df_imputed = pd.DataFrame(df_imputed, columns=columns_with_nans)

# Concating the imputed dataframe with the numeric dataframe
numeric_df = pd.concat([numeric_df,df_imputed],axis=1)

# Since all our data is numerical
relevantDataSet2 = numeric_df

# Scaling the columns to make better to prediciton
sc = StandardScaler()

# Finding scaler of relevantDataSet values without overall
scaled = sc.fit_transform(relevantDataSet2.loc[:,"potential":])
scaled

# Creating a new dataFrame of the Scaled Dataset
sub_set_data2=pd.DataFrame(scaled,columns=relevantDataSet2.loc[:,"potential":].columns)
sub_set_data2.shape

"""*Defining our xtest and ytest for further prediction from 2022*"""

xtest = sub_set_data2
ytest = relevantDataSet2['overall']

"""# **Importing modules for our training and predictions**"""

# Using ensembling on Regression
from sklearn.ensemble import VotingRegressor,RandomForestRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error
from sklearn.model_selection import train_test_split,KFold,GridSearchCV,cross_val_predict
import statsmodels.api as sm
from scipy import stats

"""**Splitting our training dataset**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

"""*Using RandomForest Regression technique*"""

random_forest = RandomForestRegressor()
svr = SVR()
xgb_model = XGBRegressor(base_score=0.5, booster='gblinear', n_estimators=100, learning_rate=0.1, random_state=42)
decision_tree = DecisionTreeRegressor(max_depth=10)

# Number of folds
cv=KFold(n_splits=2)
param_gridRandomForest  = {
    'n_estimators': [300,400,600],
    'max_depth': [100, 300],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [2, 4],
    'max_features': [ 'sqrt', 'log2']
}

param_gridSvr = {
    'C': [10, 30,60],
    'epsilon': [0.1, 0.01, 0.001],
    'kernel': ['rbf', 'sigmoid']
}

param_gridXgb = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [5, 10, 30],
    'n_estimators': [ 200, 300,1000]
}

param_gridDt = {
    'max_depth': [200,1000],
    'min_samples_leaf': [8, 16]
}

"""Structure of our model to be used in the VotingRegressor"""

voting_regressor = VotingRegressor(estimators=[
    ('random_forest', random_forest),
    ('svr', svr),
    ('xgb_model', xgb_model),
    ('decision_tree',decision_tree)
])

prediction_intervals = []

"""Performing Fits, Predicitons and finding the Mean Squared Errors(MSE) and fine tunning MSE for each regressors used on the test data from split"""

for rgr in ( random_forest,svr,xgb_model,decision_tree, voting_regressor):
 rgr.fit(x_train, y_train)
 # Fit a linear regression model
 model = sm.OLS(y_train, sm.add_constant(x_train))
 results = model.fit()
 y_pred = rgr.predict(x_test) #Predict
 mae = mean_absolute_error(y_test, y_pred) # Calculate MAE

 # Calculate the margin of error for prediction intervals
 alpha = 0.95  # Set the confidence level (e.g., 95%)
 n = len(x_train)
 mse = mean_squared_error(y_test, y_pred)
 std_err = np.sqrt(mse)
 t_score = stats.t.ppf(1 - (1 - alpha) / 2, df=n - 2)  # T-score for a two-tailed test
 margin_error = t_score * std_err

 # Calculate lower and upper bounds for the prediction interval
 lower_bound = y_pred - margin_error
 upper_bound = y_pred + margin_error

 # Append results to arrays
 prediction_intervals.append((lower_bound, upper_bound))


 print("Using the",rgr.__class__.__name__,"\n Mean Absolute Error (MAE):", mae)
 print(" Prediction Accuracy:","{:.2f}%".format(100*r2_score(y_test, y_pred)))
 print(" Prediction Interval:", (lower_bound[0], upper_bound[0]))
 print(" ")

"""Performing fits, Predictions and finding the Mean Squared Errors (MSE) and fine tuning the MSE for each regressors used on the 2022 dataset"""

prediction_intervals = []

for rgr in ( random_forest,svr,xgb_model,decision_tree, voting_regressor):
 rgr.fit(x_train, y_train)
 y_pred = rgr.predict(xtest) # Predict from the test data
 mae = mean_absolute_error(ytest, y_pred) # Calculate the Mean Absolute Error
  # Calculate the margin of error for prediction intervals
 alpha = 0.95  # Set the confidence level (e.g., 95%)
 n = len(x_train)
 mse = mean_squared_error(ytest, y_pred)
 std_err = np.sqrt(mse)
 t_score = stats.t.ppf(1 - (1 - alpha) / 2, df=n - 2)  # T-score for a two-tailed test
 margin_error = t_score * std_err

 # Calculate lower and upper bounds for the prediction interval
 lower_bound = y_pred - margin_error
 upper_bound = y_pred + margin_error

 # Append results to arrays
 prediction_intervals.append((lower_bound, upper_bound))
 print("Using the",rgr.__class__.__name__,"\n Mean Absolute Error (MAE):", mae)
 print(" Prediction Accuracy:","{:.2f}%".format(100*r2_score(ytest, y_pred)))
 print(" Prediction Interval:", (lower_bound[0], upper_bound[0]))

 print(" ")

"""Fine-tuned models

Our model instantiations
"""

fine_tuned_random_forest = RandomForestRegressor(n_estimators=400, max_depth=100, random_state=82,n_jobs=-1)
fine_tuned_svr = SVR(kernel='rbf', C=10, epsilon=0.1)
fine_tuned_xgb_model = XGBRegressor(learning_rate=0.1, max_depth=10, n_estimators=200)
fine_tuned_decision_tree = DecisionTreeRegressor(max_depth=200, min_samples_leaf=8)

grid_search_rf = GridSearchCV(fine_tuned_random_forest, param_grid=param_gridRandomForest, cv=cv, scoring="neg_mean_absolute_error")
grid_search_svr = GridSearchCV(fine_tuned_svr, param_grid=param_gridSvr, cv=cv,scoring="neg_mean_absolute_error")
grid_search_xgb = GridSearchCV(fine_tuned_xgb_model, param_grid=param_gridXgb, cv=cv,scoring="neg_mean_absolute_error")
grid_search_dt = GridSearchCV(fine_tuned_decision_tree, param_grid=param_gridDt, cv=cv,scoring="neg_mean_absolute_error")

""""**XGB**" Model has been out performing all the other models so we decided to use it for the final prediction for 2022

We chose it because it had the smallest mean absolute error

Random_forest Model optimised
"""

# Perform cross-validation and get predictions
grid_search_rf.fit(x_train, y_train)
# Get the best model from grid search
best_model = grid_search_rf.best_estimator_
# Calculate MAE and R2 score on the test set
y_pred = best_model.predict(xtest)
mae = mean_absolute_error(ytest, y_pred)
r2 = round(r2_score(ytest, y_pred))
random_forest=best_model
print("Cross validation results:")
print(" Mean Absolute Error (MAE):", mae)
print(" Prediction Accuracy (R2):","{:.2f}%".format(100* r2))

"""XGB Model optimised"""

# Perform cross-validation and get predictions
grid_search_xgb.fit(x_train, y_train)
# Get the best model from grid search
best_model = grid_search_xgb.best_estimator_
# Calculate MAE and R2 score on the test set
y_pred = best_model.predict(xtest)
mae = mean_absolute_error(ytest, y_pred)
r2 = round(r2_score(ytest, y_pred))
xgb_model=best_model
print("Cross validation results:")
print(" Mean Absolute Error (MAE):", mae)
print(" Prediction Accuracy (R2):","{:.2f}%".format(100* r2))

"""SVR Model"""

# Perform cross-validation and get predictions
grid_search_svr.fit(x_train, y_train)
# Get the best model from grid search
best_model = grid_search_svr.best_estimator_
# Calculate MAE and R2 score on the test set
y_pred = best_model.predict(xtest)
mae = mean_absolute_error(ytest, y_pred)
r2 = round(r2_score(ytest, y_pred))
svr=best_model
print("Cross validation results:")
print(" Mean Absolute Error (MAE):", mae)
print(" Prediction Accuracy (R2):","{:.2f}%".format(100* r2))

"""Decision Tree"""

grid_search_dt.fit(x_train, y_train)
# Get the best model from grid search
best_model = grid_search_dt.best_estimator_
y_pred = best_model.predict(xtest)
mae = mean_absolute_error(ytest, y_pred)
r2 = round(r2_score(ytest, y_pred))
decision_tree=best_model
print("Cross validation results:")
print(" Mean Absolute Error (MAE):", mae)
print(" Prediction Accuracy (R2):","{:.2f}%".format(100* r2))

import math

def predict(potential,attacking_short_passing,skill_long_passing,movement_reactions,power_shot_power,mentality_vision,mentality_composure,value_eur,wage_eur,passing,dribbling,physic):
  data = [potential,attacking_short_passing,skill_long_passing,movement_reactions,power_shot_power,mentality_vision,mentality_composure,value_eur,wage_eur,passing,dribbling,physic]
  if all(not math.isnan(variable) for variable in data) & all(isinstance(variable, (int, float)) for variable in data):
    scaled_data = sc.fit_transform([data])
    return round(grid_search_xgb.best_estimator_.predict(scaled_data),2)
  return 'Check your inputs, there is a mistake in there'

"""**For using the model**"""

import joblib

# Assuming 'model' is your trained machine learning model
model = grid_search_xgb.best_estimator_

# Specify the path in your Google Drive where you want to save the model
save_path = "/content/drive/My Drive/AI/SavedModels/sports.joblib"

# Save the model to the specified path
joblib.dump(model, save_path)

with open("/content/drive/My Drive/AI/SavedModels/sport_model2.sav", 'wb') as model_file:
    pickle.dump((xtrain21scaled,xgb_model,random_forest), model_file)

print(f"Model saved as {model_file}")

